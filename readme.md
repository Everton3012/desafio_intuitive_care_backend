# Teste TÃ©cnico - EstÃ¡gio IntuitiveCare 2026

## VisÃ£o Geral

Projeto desenvolvido em Python para coleta, processamento, consolidaÃ§Ã£o, validaÃ§Ã£o,
enriquecimento e agregaÃ§Ã£o de dados de despesas das operadoras de saÃºde a partir da
API de Dados Abertos da ANS.

O projeto foi implementado como um **pipeline automatizado ponta a ponta**, cobrindo integralmente os Testes 1, 2, 3 e 4 do desafio tÃ©cnico.

---

## RepositÃ³rios Relacionados

- **Backend (este repositÃ³rio):** API FastAPI + Pipeline ETL
- **Frontend:** [link-do-repositorio-frontend](https://github.com/Everton3012/front_intuitive_care)

---

## Fonte dos Dados

Os dados utilizados no projeto sÃ£o provenientes da base pÃºblica da ANS
(AgÃªncia Nacional de SaÃºde Suplementar):

https://dadosabertos.ans.gov.br/FTP/PDA/

Fontes utilizadas:

- DemonstraÃ§Ãµes ContÃ¡beis Trimestrais (Ãºltimos 3 trimestres disponÃ­veis)
- Cadastro de Operadoras de Planos de SaÃºde **Ativas**
- Cadastro de Operadoras de Planos de SaÃºde **Canceladas**

---

## Tecnologias

- Python 3.10+
- Pandas
- Requests
- BeautifulSoup4
- FastAPI
- PostgreSQL 15
- Docker & Docker Compose
- SQL
- Git

---

## Estrutura do Projeto

```
Teste_IntuitiveCare/
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ download_ans.py
â”‚   â”œâ”€â”€ download_operadoras.py
â”‚   â”œâ”€â”€ process_files.py
â”‚   â”œâ”€â”€ consolidate.py
â”‚   â””â”€â”€ validate_and_aggregate.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ extracted/
â”‚   â””â”€â”€ final/
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ etl.log
â”‚   â””â”€â”€ validation.log
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ db.py
â”‚   â”œâ”€â”€ queries.py
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_ddl.sql
â”‚   â”œâ”€â”€ 02_import.sql
â”‚   â””â”€â”€ 03_queries.sql
â”‚
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ run_pipeline.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## ExecuÃ§Ã£o RÃ¡pida (Docker Compose)

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- Git

### 1) Clonar o repositÃ³rio

```bash
git clone <url-do-repositorio>
cd desafio
```
### 2) Configurar variÃ¡veis de ambiente
```bash
cp .env.example .env
```

### 3) Subir os containers
```bash
docker compose up --build
```
---

## Como Executar

### 1. Criar ambiente virtual

```bash
python -m venv .venv
```

### 2. Ativar ambiente virtual

Windows:
```bash
.venv\Scripts\activate
```

Linux/Mac:
```bash
source .venv/bin/activate
```

### 3. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

---

## ExecuÃ§Ã£o do Pipeline Completo (Recomendado)

```bash
python run_pipeline.py
```

Esse comando executa automaticamente:

1. Download dos cadastros de operadoras (ativas e canceladas)
2. Download dos Ãºltimos 3 arquivos trimestrais de demonstraÃ§Ãµes contÃ¡beis
3. ExtraÃ§Ã£o e processamento dos arquivos
4. ConsolidaÃ§Ã£o com dados cadastrais
5. ValidaÃ§Ã£o, enriquecimento e agregaÃ§Ã£o final
6. GeraÃ§Ã£o do arquivo ZIP final exigido no teste

---

## ExecuÃ§Ã£o por Etapas (Opcional)

### Download das DemonstraÃ§Ãµes ContÃ¡beis

```bash
python etl/download_ans.py
```

### Download do Cadastro de Operadoras

```bash
python etl/download_operadoras.py
```
### Processamento e ConsolidaÃ§Ã£o Inicial (ETL)

```bash
python etl/process_files.py
```

Gera:
```
data/final/despesas_por_operadora_trimestre.csv
```

### ConsolidaÃ§Ã£o com Dados Cadastrais

```bash
python etl/consolidate.py
```

Gera:
```
data/final/despesas_consolidadas_final.csv
data/final/consolidado_despesas.zip
```

### ValidaÃ§Ã£o, Enriquecimento e AgregaÃ§Ã£o (Teste 2)

```bash
python etl/validate_and_aggregate.py
```

Gera:
```
data/final/despesas_agregadas.csv
Teste_Everton_Brandao.zip
```

---

## Executar Banco (PostgreSQL via Docker) + Importar CSVs

> PrÃ©-requisito: Docker instalado.

### 1) Subir o container PostgreSQL

Exemplo:

```bash
docker run --name intuitivecare_postgres -e POSTGRES_DB=intuitivecare -e POSTGRES_USER=intuitive -e POSTGRES_PASSWORD=intuitive123 -p 5432:5432 -d postgres:15
```
### 2) Copiar CSVs gerados para dentro do container

```bash
docker cp data/final/despesas_consolidadas_final.csv intuitivecare_postgres:/tmp/despesas_consolidadas_final.csv
docker cp data/final/despesas_agregadas.csv intuitivecare_postgres:/tmp/despesas_agregadas.csv
docker cp data/raw/Relatorio_cadop.csv intuitivecare_postgres:/tmp/Relatorio_cadop.csv
docker cp data/raw/Relatorio_cadop_canceladas.csv intuitivecare_postgres:/tmp/Relatorio_cadop_canceladas.csv
```

### 3) Rodar DDL e Import
```bash
docker exec -i intuitivecare_postgres psql -U intuitive -d intuitivecare -v ON_ERROR_STOP=1 < sql/01_ddl.sql
docker exec -i intuitivecare_postgres psql -U intuitive -d intuitivecare -v ON_ERROR_STOP=1 < sql/02_import.sql
```
### 4) Validar carga

```bash
docker exec -it intuitivecare_postgres psql -U intuitive -d intuitivecare -c "SELECT COUNT(*) operadoras FROM operadoras; SELECT COUNT(*) despesas FROM despesas_consolidadas; SELECT COUNT(*) agregadas FROM despesas_agregadas;"
```

---

## Executar API (FastAPI)

### 1) Criar arquivo `.env` na raiz

Use o `.env.example` como base e crie um `.env` com:

```env
DB_HOST=localhost
DB_PORT=5432
DB_NAME=intuitivecare
DB_USER=intuitive
DB_PASSWORD=intuitive123
```

### 2) Subir o servidor

```bash
uvicorn api.main:app --host 127.0.0.1 --port 8000 --reload
```

### 3) Acessar documentaÃ§Ã£o (Swagger)

- http://127.0.0.1:8000/docs

### 4) Teste rÃ¡pido

```bash
curl -X GET "http://127.0.0.1:8000/health"
curl -X GET "http://127.0.0.1:8000/api/operadoras?page=1&limit=10"
curl -X GET "http://127.0.0.1:8000/api/estatisticas"
```

---

## Teste 2 â€“ TransformaÃ§Ã£o e ValidaÃ§Ã£o de Dados

### ValidaÃ§Ãµes Implementadas

- CNPJ vÃ¡lido (formato + dÃ­gitos verificadores)
- RazÃ£o Social nÃ£o vazia
- Valores numÃ©ricos nÃ£o negativos

#### EstratÃ©gia para CNPJs InvÃ¡lidos

**DecisÃ£o adotada:** descarte dos registros com CNPJ invÃ¡lido.

**PrÃ³s:**
- Evita inconsistÃªncias no enriquecimento
- Garante integridade das agregaÃ§Ãµes
- Simplifica o pipeline

**Contras:**
- ReduÃ§Ã£o do volume final de dados
- PossÃ­vel perda de registros com erro de origem

---

### Enriquecimento de Dados

- Join realizado via `CNPJ`
- Cadastros de operadoras ativas e canceladas sÃ£o unificados
- Registros sem correspondÃªncia no cadastro sÃ£o mantidos com campos nulos
- Duplicidades de CNPJ sÃ£o resolvidas via `drop_duplicates`

Colunas adicionadas:
- RegistroANS
- Modalidade
- UF

---

### AgregaÃ§Ãµes Realizadas

Agrupamento por:
- RazaoSocial
- UF

MÃ©tricas calculadas:
- Total de despesas
- MÃ©dia trimestral
- Desvio padrÃ£o

OrdenaÃ§Ã£o:
- Total de despesas (ordem decrescente)

---

## Teste 3 â€“ Banco de Dados e AnÃ¡lise (PostgreSQL)

### Modelagem e NormalizaÃ§Ã£o

Foi adotada uma abordagem de **modelagem normalizada**, com tabelas separadas para:

- Operadoras
- Despesas consolidadas
- Despesas agregadas

Essa decisÃ£o permite evitar redundÃ¢ncia de dados cadastrais, manter integridade
referencial e facilitar anÃ¡lises futuras com menor risco de inconsistÃªncias.


### Trade-offs TÃ©cnicos â€“ NormalizaÃ§Ã£o

**Abordagem escolhida:** Tabelas normalizadas (OpÃ§Ã£o B)

**Justificativa:**

- O volume de dados Ã© moderado e adequado para normalizaÃ§Ã£o sem impacto negativo
  relevante de performance.
- A frequÃªncia de atualizaÃ§Ã£o dos dados Ã© baixa (processamento periÃ³dico),
  reduzindo o custo de joins.
- Queries analÃ­ticas se tornam mais legÃ­veis e fÃ¡ceis de manter.
- Evita duplicaÃ§Ã£o de informaÃ§Ãµes cadastrais das operadoras.

**Alternativa considerada (tabela desnormalizada):**
Foi descartada por aumentar redundÃ¢ncia e dificultar a manutenÃ§Ã£o dos dados
cadastrais ao longo do tempo.

### Tipos de Dados

**Valores monetÃ¡rios:**
Foi utilizado o tipo `DECIMAL` em vez de `FLOAT`, garantindo precisÃ£o nos cÃ¡lculos
financeiros e evitando erros de arredondamento.

**Datas e perÃ­odos:**
- Ano e trimestre foram armazenados separadamente (`SMALLINT`), pois:
  - Facilitam filtros e agregaÃ§Ãµes
  - Evitam parsing de strings
  - SÃ£o suficientes para o nÃ­vel de granularidade do problema

**CNPJ e UF:**
- `VARCHAR` para CNPJ, preservando zeros Ã  esquerda
- `CHAR(2)` para UF, garantindo padronizaÃ§Ã£o

### ImportaÃ§Ã£o e Tratamento de InconsistÃªncias

A importaÃ§Ã£o dos dados foi realizada utilizando **tabelas de staging**, permitindo
tratamento prÃ©vio antes da inserÃ§Ã£o nas tabelas finais.

Durante o processo, foram tratadas as seguintes situaÃ§Ãµes:

- **Encoding:**  
  Arquivos das operadoras utilizam `LATIN1`, enquanto os arquivos de despesas
  utilizam `UTF-8`. O encoding foi tratado explicitamente durante o `COPY`.

- **Valores NULL em campos obrigatÃ³rios:**  
  Registros sem CNPJ, RazÃ£o Social ou valores numÃ©ricos vÃ¡lidos foram descartados.

- **Strings em campos numÃ©ricos:**  
  Valores monetÃ¡rios foram normalizados removendo separadores de milhar e
  convertidos para `DECIMAL`.

- **Duplicidade de CNPJ:**  
  Foi priorizada a operadora ativa em caso de duplicidade, mantendo apenas um
  registro por CNPJ.

Essa abordagem garante maior robustez e evita falhas durante a carga.

### Queries AnalÃ­ticas

Foram desenvolvidas queries analÃ­ticas utilizando **CTEs** e **window functions**
para maior clareza e desempenho.

**Query 1 â€“ Crescimento percentual de despesas:**
- Calcula o crescimento entre o primeiro e o Ãºltimo trimestre disponÃ­vel por
  operadora.
- Operadoras sem dados completos sÃ£o mantidas, mas excluÃ­das do ranking caso nÃ£o
  seja possÃ­vel calcular o crescimento.

**Query 2 â€“ DistribuiÃ§Ã£o de despesas por UF:**
- Soma total de despesas por UF
- CÃ¡lculo da mÃ©dia de despesas por operadora em cada estado

**Query 3 â€“ Operadoras acima da mÃ©dia:**
- Identifica operadoras com despesas acima da mÃ©dia geral em pelo menos 2 dos 3
  trimestres analisados.
- Foi escolhida uma abordagem baseada em agregaÃ§Ãµes e CTEs por oferecer boa
  legibilidade e facilidade de manutenÃ§Ã£o.

---

## AtualizaÃ§Ã£o dos dados (novos trimestres)

O pipeline sempre baixa automaticamente os **Ãºltimos 3 trimestres disponÃ­veis** na ANS.
Quando um novo trimestre Ã© publicado, basta executar novamente:

```bash
python run_pipeline.py
```
Depois, reimportar os CSVs no PostgreSQL:

```bash
docker exec -i intuitivecare_postgres psql -U intuitive -d intuitivecare -v ON_ERROR_STOP=1 < sql/02_import.sql
```
A estratÃ©gia adotada no import Ã© `TRUNCATE + INSERT`, garantindo consistÃªncia e simplicidade (KISS),
jÃ¡ que o volume Ã© moderado.

---

## Teste 4 â€“ API (FastAPI)

Foi implementada uma API em FastAPI para consulta das operadoras e despesas consolidadas
a partir do banco PostgreSQL gerado no Teste 3.

### Rotas implementadas

- `GET /api/operadoras`  
  Lista operadoras com paginaÃ§Ã£o (`page`, `limit`) , filtro opcional `q` (CNPJ ou RazÃ£o Social) e filtro opcional `situacao` (ATIVA ou CANCELADA).

- `GET /api/operadoras/{cnpj}`  
  Retorna detalhes de uma operadora especÃ­fica.

- `GET /api/operadoras/{cnpj}/despesas`  
  Retorna o histÃ³rico de despesas da operadora nos 3 trimestres analisados.

- `GET /api/estatisticas`  
  Retorna estatÃ­sticas agregadas: total, mÃ©dia, top 5 operadoras e top 5 UFs por despesas.

- `GET /health`  
  Healthcheck simples com verificaÃ§Ã£o de conexÃ£o ao banco.

- `POST /api/admin/atualizar`  
  executa a pipeline ,sobe as informaÃ§Ãµes para o banco e devolve as nformaÃ§oes para o frontend

### Trade-offs TÃ©cnicos (Backend)

**Framework:** FastAPI  
Escolhi FastAPI por oferecer tipagem, validaÃ§Ã£o automÃ¡tica (Pydantic), Swagger/OpenAPI nativo,
boa performance e facilidade de manutenÃ§Ã£o.

**PaginaÃ§Ã£o:** Offset-based (`page/limit` com `OFFSET/LIMIT`)  
Escolhida por simplicidade (KISS) e por o volume ser baixo/moderado (~4k operadoras).  
Para grandes volumes, seria melhor Keyset/Cursor pagination.

**/api/estatisticas:** queries diretas  
As estatÃ­sticas sÃ£o calculadas via SQL no momento da requisiÃ§Ã£o, pois os dados mudam apenas
quando o pipeline Ã© executado. Em cenÃ¡rio real, poderia ser cacheado por X minutos ou prÃ©-calculado.

**Resposta de paginaÃ§Ã£o:** dados + metadados  
Retorna `{ data, total, page, limit }` para facilitar o frontend e evitar chamadas extras.

---

## ðŸ§© VisÃ£o Geral da Arquitetura

```
Dados PÃºblicos ANS
        â†“
Pipeline Python (ETL)
        â†“
CSVs Consolidados
        â†“
PostgreSQL (Docker)
        â†“
FastAPI
        â†“
Frontend (Vue.js)
```
---

## Logs

- `logs/etl.log`: download, extraÃ§Ã£o, processamento e consolidaÃ§Ã£o
- `logs/validation.log`: validaÃ§Ãµes, enriquecimento e agregaÃ§Ãµes

---

## Resultado Final

Arquivos gerados:

- `despesas_por_operadora_trimestre.csv`
- `despesas_consolidadas_final.csv`
- `despesas_agregadas.csv`
- `Teste_Everton_Brandao.zip`

---

## Autor

Everton BrandÃ£o